import os
import pandas as pd
import joblib

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score


def load_dataset(path):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î CSV –∑ —Ç–µ–∫—Å—Ç–∞–º–∏ —Ç–∞ –º—ñ—Ç–∫–∞–º–∏.
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—É —ñ –±–∞–∑–æ–≤—É –≤–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {path}")

    df = pd.read_csv(path)

    # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ –∞–±–æ –±–∏—Ç—ñ —Ä—è–¥–∫–∏
    df = df.dropna(subset=["text", "label"])

    if len(df) < 100:
        raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–∏–π –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ")

    return df


def build_vectorizer():
    """
    –°—Ç–≤–æ—Ä—é—î TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    –ë—ñ–≥—Ä–∞–º–∏ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤–ª–æ–≤–ª—é–≤–∞—Ç–∏ —Å—Ç–∏–ª—ñ—Å—Ç–∏—á–Ω—ñ —à–∞–±–ª–æ–Ω–∏.
    """
    return TfidfVectorizer(
        max_features=12000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.95,
        strip_accents="unicode"
    )


def build_model():
    """
    –õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è ‚Äî —Å—Ç–∞–±—ñ–ª—å–Ω–∞, —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å
    –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—ñ–≤.
    """
    return LogisticRegression(
        max_iter=1500,
        class_weight="balanced",
        n_jobs=-1
    )


def train_and_evaluate(X, y, vectorizer, model):
    """
    –î—ñ–ª–∏—Ç—å –¥–∞–Ω—ñ, —Ç—Ä–µ–Ω—É—î –º–æ–¥–µ–ª—å —ñ –ø–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
    """
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )

    print("üß† –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    model.fit(X_train, y_train)

    print("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ...")
    predictions = model.predict(X_test)

    accuracy = accuracy_score(y_test, predictions)
    report = classification_report(y_test, predictions)

    return accuracy, report


def save_artifacts(model, vectorizer, folder):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –º–æ–¥–µ–ª—å —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä.
    """
    model_path = os.path.join(folder, "text_model.pkl")
    vectorizer_path = os.path.join(folder, "text_vectorizer.pkl")

    joblib.dump(model, model_path)
    joblib.dump(vectorizer, vectorizer_path)

    print(f"üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {model_path}")
    print(f"üíæ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑–±–µ—Ä–µ–∂–µ–Ω–∏–π: {vectorizer_path}")


def main():
    base_dir = os.path.dirname(os.path.abspath(__file__))
    dataset_path = os.path.join(base_dir, "dataset_text.csv")

    print("üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É...")
    try:
        df = load_dataset(dataset_path)
    except Exception as e:
        print(f"üõë –ü–æ–º–∏–ª–∫–∞: {e}")
        return

    print(f"üìä –¢–µ–∫—Å—Ç—ñ–≤ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ: {len(df)}")

    vectorizer = build_vectorizer()
    print("üî¢ –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ–∫—Å—Ç —É –≤–µ–∫—Ç–æ—Ä–∏...")
    X = vectorizer.fit_transform(df["text"])
    y = df["label"]

    model = build_model()

    accuracy, report = train_and_evaluate(X, y, vectorizer, model)

    print("\n" + "=" * 40)
    print("‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print(f"üéØ –¢–æ—á–Ω—ñ—Å—Ç—å: {accuracy:.2%}")
    print("=" * 40)
    print(report)

    save_artifacts(model, vectorizer, base_dir)


if __name__ == "__main__":
    main()
